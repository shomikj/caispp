{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional Neural Networks - Cats vs Dogs: CNN model to classify cats vs dogs\n",
    "# Shomik Jain, USC CAIS++\n",
    "\n",
    "# References: Dataset -- https://www.kaggle.com/c/dogs-vs-cats/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional Neural Networks\n",
    "# stack successive layers of convolution & max pooling\n",
    "    # results in a low dimensional, meaningful signal related to certain abstract features detected\n",
    "# decision network: fully connected layers -- make decision based on signal produced by conv layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional Layers: scan input for structured features\n",
    "# convolution: mathematical operation used to extract features from an image\n",
    "    # image kernal matrix: applied to image by sliding over the image \n",
    "      # and for each position, computing element wise multiplication and summing result\n",
    "# CNNs learn values of kernal filters, stack multiple layers of feature detectors (kernals)\n",
    "  # on top of eachother for abstracted levels of feature detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.ndimage import imread\n",
    "import cv2\n",
    "import sklearn.utils\n",
    "\n",
    "DATA_PATH = './data/train/'\n",
    "TEST_PERCENT = 0.1\n",
    "# amount of dataset to use (for training time)\n",
    "SELECT_SUBSET_PERCENT = 1.0\n",
    "\n",
    "# The cat and dog images are of variable size we have to resize them to all the same size.\n",
    "RESIZE_WIDTH=64\n",
    "RESIZE_HEIGHT=64\n",
    "\n",
    "EPOCHS = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to load 25000 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shomikj/anaconda3/envs/caispp/lib/python3.5/site-packages/ipykernel/__main__.py:21: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0.\n",
      "Use ``matplotlib.pyplot.imread`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have loaded 1000 samples\n",
      "Have loaded 2000 samples\n",
      "Have loaded 3000 samples\n",
      "Have loaded 4000 samples\n",
      "Have loaded 5000 samples\n",
      "Have loaded 6000 samples\n",
      "Have loaded 7000 samples\n",
      "Have loaded 8000 samples\n",
      "Have loaded 9000 samples\n",
      "Have loaded 10000 samples\n",
      "Have loaded 11000 samples\n",
      "Have loaded 12000 samples\n",
      "Have loaded 13000 samples\n",
      "Have loaded 14000 samples\n",
      "Have loaded 15000 samples\n",
      "Have loaded 16000 samples\n",
      "Have loaded 17000 samples\n",
      "Have loaded 18000 samples\n",
      "Have loaded 19000 samples\n",
      "Have loaded 20000 samples\n",
      "Have loaded 21000 samples\n",
      "Have loaded 22000 samples\n",
      "Have loaded 23000 samples\n",
      "Have loaded 24000 samples\n",
      "Train set has dimensionality (22500, 64, 64, 3)\n",
      "Test set has dimensionality (2500, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "# 1. Load the Data\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "files = os.listdir(DATA_PATH)\n",
    "# Shuffle to select about an equal number of dog and cat images.\n",
    "shuffled_files = sklearn.utils.shuffle(files)\n",
    "select_count = int(len(shuffled_files) * SELECT_SUBSET_PERCENT)\n",
    "\n",
    "print('Going to load %i files' % select_count)\n",
    "\n",
    "subset_files_select = shuffled_files[:select_count]\n",
    "\n",
    "DISPLAY_COUNT = 1000\n",
    "\n",
    "for i, input_file in enumerate(subset_files_select):\n",
    "    if i % DISPLAY_COUNT == 0 and i != 0:\n",
    "        print('Have loaded %i samples' % i)\n",
    "        \n",
    "    img = imread(DATA_PATH + input_file)\n",
    "    # Resize the images to be the same size.\n",
    "    img = cv2.resize(img, (RESIZE_WIDTH, RESIZE_HEIGHT), interpolation=cv2.INTER_CUBIC)\n",
    "    X.append(img)\n",
    "    if 'cat' == input_file.split('.')[0]:\n",
    "        Y.append(0.0)\n",
    "    else:\n",
    "        Y.append(1.0)\n",
    "        \n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "\n",
    "test_size = int(len(X) * TEST_PERCENT)\n",
    "\n",
    "test_X = X[:test_size]\n",
    "test_Y = Y[:test_size]\n",
    "train_X = X[test_size:]\n",
    "train_Y = Y[test_size:]\n",
    "\n",
    "print('Train set has dimensionality %s' % str(train_X.shape))\n",
    "print('Test set has dimensionality %s' % str(test_X.shape))\n",
    "\n",
    "# Normalization\n",
    "train_X = train_X.astype('float32')\n",
    "test_X = test_X.astype('float32')\n",
    "train_X /= 255\n",
    "test_X /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Defining the Network \n",
    "\n",
    "# References\n",
    "# Convolution layers: https://keras.io/layers/convolutional/\n",
    "# Batch norm layer: https://keras.io/layers/normalization/\n",
    "# Layer initializers: https://keras.io/initializers/\n",
    "# Dense layer: https://keras.io/layers/core/#dense\n",
    "# Activation functions: https://keras.io/layers/core/#activation\n",
    "# Regulizers:\n",
    "    # https://keras.io/layers/core/#dropout\n",
    "    # https://keras.io/regularizers/\n",
    "    # https://keras.io/callbacks/#earlystopping\n",
    "    # https://keras.io/constraints/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary layers.\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Activation, MaxPooling2D, Dropout, Flatten, BatchNormalization\n",
    "from keras import optimizers\n",
    "from keras import losses\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# stack successive layers of convolution and max pooling\n",
    "# results in a low dimensional, meaningful signal related to certain abstract features detected\n",
    "model.add(Conv2D(input_shape=(64, 64, 3), filters = 16, kernel_size = 4))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(filters = 32, kernel_size = 4))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(filters = 64, kernel_size = 4))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size = 2))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "# decision network: fully connected layers \n",
    "# make decision based on signal produced by convolution layers\n",
    "model.add(Dense(128))\n",
    "model.add(Activation(\"relu\"))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(units=1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "# Cost Function: measure \n",
    "optimizer = optimizers.RMSprop(lr=5e-4)\n",
    "loss = 'binary_crossentropy'\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18000 samples, validate on 4500 samples\n",
      "Epoch 1/9\n",
      "18000/18000 [==============================] - 113s 6ms/step - loss: 0.6362 - acc: 0.6504 - val_loss: 0.5817 - val_acc: 0.6909\n",
      "Epoch 2/9\n",
      "18000/18000 [==============================] - 110s 6ms/step - loss: 0.5359 - acc: 0.7334 - val_loss: 0.5006 - val_acc: 0.7516\n",
      "Epoch 3/9\n",
      "18000/18000 [==============================] - 109s 6ms/step - loss: 0.4757 - acc: 0.7745 - val_loss: 0.4358 - val_acc: 0.7976\n",
      "Epoch 4/9\n",
      "18000/18000 [==============================] - 108s 6ms/step - loss: 0.4220 - acc: 0.8070 - val_loss: 0.4489 - val_acc: 0.7989\n",
      "Epoch 5/9\n",
      "18000/18000 [==============================] - 108s 6ms/step - loss: 0.3813 - acc: 0.8295 - val_loss: 0.4340 - val_acc: 0.8038\n",
      "Epoch 6/9\n",
      "18000/18000 [==============================] - 108s 6ms/step - loss: 0.3376 - acc: 0.8507 - val_loss: 0.4681 - val_acc: 0.7951\n",
      "Epoch 7/9\n",
      "18000/18000 [==============================] - 118s 7ms/step - loss: 0.2973 - acc: 0.8729 - val_loss: 0.3733 - val_acc: 0.8338\n",
      "Epoch 8/9\n",
      "18000/18000 [==============================] - 105s 6ms/step - loss: 0.2589 - acc: 0.8917 - val_loss: 0.3989 - val_acc: 0.8371\n",
      "Epoch 9/9\n",
      "18000/18000 [==============================] - 109s 6ms/step - loss: 0.2225 - acc: 0.9072 - val_loss: 0.5725 - val_acc: 0.7931\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x138564278>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the batch size\n",
    "batch_size = 32\n",
    "\n",
    "model.fit(train_X, train_Y, batch_size=batch_size, epochs=EPOCHS, validation_split=0.2, verbose=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500/2500 [==============================] - 5s 2ms/step\n",
      "\n",
      "Got 81.32% accuracy\n"
     ]
    }
   ],
   "source": [
    "# 3. Evaluate the Model\n",
    "\n",
    "loss, acc = model.evaluate(test_X, test_Y, batch_size=batch_size, verbose=1)\n",
    "\n",
    "print('')\n",
    "print('Got %.2f%% accuracy' % (acc * 100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:caispp]",
   "language": "python",
   "name": "conda-env-caispp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
