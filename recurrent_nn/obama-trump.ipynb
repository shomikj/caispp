{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recurrent Neural Networks - Obama vs Trump: RNN model to classify between Obama/Trump tweets\n",
    "# Shomik Jain, USC CAIS++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recurrent Neural Networks (RNNs)\n",
    "# designed to work on temporal/sequential data\n",
    "# RNN neurons have \"memory\" which caputres info about what has been calculated so far \n",
    "# limitations: cannot handle long term dependencies, suffer from vanishing gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Long Short Term Memory Network: LSTM\n",
    "# manage cell state with a variety of gates to more effectively control flow of info through time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load in the Data\n",
    "\n",
    "TWEETS_DIR = './obama-trump-data/tweets'\n",
    "LABELS_DIR = './obama-trump-data/labels_np'\n",
    "EMBEDDINGS_DIR = './word_vectors.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet:  \"America should be very proud.\" President Obama #LoveWins . Label:  0\n",
      "Tweet:  TO ALL AMERICANS\n",
      "https://t.co/D7Es6ie4fY . Label:  1\n",
      "Tweet:  No deal was made last night on DACA. Massive border security would have to be agreed to in exchange for consent. Would be subject to vote. . Label:  1\n",
      "Tweet:  I was viciously attacked by Mr. Khan at the Democratic Convention. Am I not allowed to respond? Hillary voted for the Iraq war, not me! . Label:  1\n",
      "Tweet:  .@RudyGiuliani, one of the finest people I know and a former GREAT Mayor of N.Y.C., just took himself out of consideration for \"State\". . Label:  1\n",
      "Tweet:  The #TPP establishes the highest labor standards of any trade agreement in history. http://t.co/3vqoancyYp . Label:  0\n",
      "Tweet:  Thoughts and prayers to the great people of Indiana. You will prevail! . Label:  1\n",
      "Tweet:  Hillary said she was under sniper fire (while surrounded by USSS.) Turned out to be a total lie. She is not fit to https://t.co/hBIrGj21l6 . Label:  1\n",
      "Tweet:  The failing @nytimes has become a newspaper of fiction. Their stories about me always quote non-existent unnamed sources. Very dishonest! . Label:  1\n",
      "Tweet:  Thank you Council Bluffs, Iowa! Will be back soon. Remember- everything you need to know about Hillary -- just https://t.co/45kIHxdX83 . Label:  1\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Load tweets, labels\n",
    "tweets = pickle.load(open(TWEETS_DIR,'rb'))\n",
    "labels = pickle.load(open(LABELS_DIR,'rb'))\n",
    "\n",
    "# Sample some tweets to display\n",
    "for i in range(0,100,10):\n",
    "    print(\"Tweet: \", tweets[i], \". Label: \", labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Data Preprocessing\n",
    "\n",
    "# 2.1 -- Tokenize the tweets: convert each tweet into a sequence of word indices\n",
    "# 2.2 -- Pad the input sequences with blank spots at end to make sure they're all same length\n",
    "# 2.3 -- Load in pre-trained word embeddings to convert each word index=>unique word embedding vector\n",
    "\n",
    "# Word embeddings are a way of encoding words into n-dimensional vectors with continuous values\n",
    "# so that the vector contains some information about the words' meanings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13049 unique tokens (words).\n",
      "Training data size is (6136,31)\n",
      "Labels are size 6136\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Tokenize the tweets (convert sentence to sequence of words)\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(tweets)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(tweets)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "print('Found %s unique tokens (words).' % len(word_index))\n",
    "\n",
    "# Pad sequences to ensure samples are the same size\n",
    "training_data = pad_sequences(sequences)\n",
    "\n",
    "print(\"Training data size is (%d,%d)\"  % training_data.shape) # shape = (num. tweets, max. length of tweet)\n",
    "print(\"Labels are size %d\"  % labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tweet:  \"America should be very proud.\" President Obama #LoveWins\n",
      "Tweet after tokenization and padding:  [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0  44  88  21  83 391  12  14 794]\n"
     ]
    }
   ],
   "source": [
    "# Show effect of tokenization, padding\n",
    "print(\"Original tweet: \", tweets[0])\n",
    "print(\"Tweet after tokenization and padding: \", training_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n",
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# Convert words to word embedding vectors\n",
    "\n",
    "EMBEDDING_DIM = 50\n",
    "print('Indexing word vectors.')\n",
    "\n",
    "import os\n",
    "embeddings_index = {}\n",
    "f = open(EMBEDDINGS_DIR)\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "# prepare word embedding matrix\n",
    "num_words = len(word_index)+1\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= num_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2645\n",
      "[ 0.079084   -0.81503999  1.79009998  0.91653001  0.10797    -0.55628002\n",
      " -0.84426999 -1.49510002  0.13417999  0.63626999  0.35146001  0.25813001\n",
      " -0.55028999  0.51055998  0.37408999  0.12092    -1.61660004  0.83653003\n",
      "  0.14202    -0.52348     0.73452997  0.12207    -0.49079001  0.32532999\n",
      "  0.45306    -1.58500004 -0.63848001 -1.00530005  0.10454    -0.42984\n",
      "  3.18099999 -0.62186998  0.16819    -1.01390004  0.064058    0.57844001\n",
      " -0.45559999  0.73782998  0.37202999 -0.57722002  0.66441     0.055129\n",
      "  0.037891    1.32749999  0.30991     0.50696999  1.23570001  0.1274\n",
      " -0.11434     0.20709001]\n"
     ]
    }
   ],
   "source": [
    "# Sample word embedding vector:\n",
    "print(word_index[\"computer\"]) # retrieve word index\n",
    "print(embedding_matrix[2645]) # use word index to retrieve word embedding vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Create the classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Input\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers.core import Dense, Activation, Flatten\n",
    "from keras.layers import Dropout, concatenate\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import metrics\n",
    "from keras.models import Model\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 31, 50)            652500    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                29440     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 684,053\n",
      "Trainable params: 31,553\n",
      "Non-trainable params: 652,500\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Add pre-trained embedding layer \n",
    "# Converts word indices to GloVe word embedding vectors as they're fed in\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                    EMBEDDING_DIM,\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=training_data.shape[1],\n",
    "                    trainable=False))\n",
    "\n",
    "# At this point, each individual training sample is now a sequence of word embedding vectors\n",
    "\n",
    "model.add(LSTM(64, return_sequences = False)) #return_sequences: if multiple LSTM layers\n",
    "model.add(Dropout(.2))\n",
    "\n",
    "# Dense 1\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(.2))\n",
    "\n",
    "# Dense 2 (final vote)\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# Define Loss and Optimizer\n",
    "LOSS = 'binary_crossentropy' #classifying between 0 and 1\n",
    "OPTIMIZER = 'rmsprop' #RMSprop typically works better for RNNs per Keras\n",
    "\n",
    "model.compile(loss = LOSS, optimizer = OPTIMIZER, metrics = [metrics.binary_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Train/Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4908 samples, validate on 1228 samples\n",
      "Epoch 1/10\n",
      "4908/4908 [==============================] - 3s 622us/step - loss: 0.5055 - binary_accuracy: 0.7569 - val_loss: 0.3589 - val_binary_accuracy: 0.8485\n",
      "Epoch 2/10\n",
      "4908/4908 [==============================] - 2s 340us/step - loss: 0.3296 - binary_accuracy: 0.8659 - val_loss: 0.5451 - val_binary_accuracy: 0.7565\n",
      "Epoch 3/10\n",
      "4908/4908 [==============================] - 2s 344us/step - loss: 0.2855 - binary_accuracy: 0.8877 - val_loss: 0.2814 - val_binary_accuracy: 0.8738\n",
      "Epoch 4/10\n",
      "4908/4908 [==============================] - 2s 342us/step - loss: 0.2449 - binary_accuracy: 0.9026 - val_loss: 0.2505 - val_binary_accuracy: 0.8974\n",
      "Epoch 5/10\n",
      "4908/4908 [==============================] - 2s 351us/step - loss: 0.2185 - binary_accuracy: 0.9173 - val_loss: 0.2030 - val_binary_accuracy: 0.9243\n",
      "Epoch 6/10\n",
      "4908/4908 [==============================] - 2s 346us/step - loss: 0.2052 - binary_accuracy: 0.9201 - val_loss: 0.2049 - val_binary_accuracy: 0.9112\n",
      "Epoch 7/10\n",
      "4908/4908 [==============================] - 2s 342us/step - loss: 0.1822 - binary_accuracy: 0.9319 - val_loss: 0.1858 - val_binary_accuracy: 0.9324\n",
      "Epoch 8/10\n",
      "4908/4908 [==============================] - 2s 343us/step - loss: 0.1735 - binary_accuracy: 0.9374 - val_loss: 0.1794 - val_binary_accuracy: 0.9292\n",
      "Epoch 9/10\n",
      "4908/4908 [==============================] - 2s 348us/step - loss: 0.1630 - binary_accuracy: 0.9374 - val_loss: 0.2075 - val_binary_accuracy: 0.9218\n",
      "Epoch 10/10\n",
      "4908/4908 [==============================] - 2s 343us/step - loss: 0.1492 - binary_accuracy: 0.9446 - val_loss: 0.1877 - val_binary_accuracy: 0.9373\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1243b7048>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of epochs and batch size\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "model.fit(training_data, labels, \n",
    "          epochs = EPOCHS, \n",
    "          batch_size = BATCH_SIZE, \n",
    "          validation_split =.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:caispp]",
   "language": "python",
   "name": "conda-env-caispp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
